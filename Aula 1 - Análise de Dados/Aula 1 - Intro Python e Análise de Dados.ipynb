{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uma (muito breve) introdução à Python\n",
    "\n",
    "![Python logo, disponível em: https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png](https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png)\n",
    "\n",
    "Python é a linguagem \"queridinha\" dos Cientistas de Dados devido a seu catálogo vasto de bibliotecas de manipulação de dados, matemática e frameworks que abstraem muitos detalhes de programação. Além disso, a integração com as bibliotecas é extremamente simples, e sua sintaxe é simples e fácil de aprender. \n",
    "\n",
    "\n",
    "## Variáveis\n",
    "Python é uma linguagem **fortemente tipada**, ou seja, faz uso dos tipos convencionais (int, float, char, etc.), porém ela é também **dinamicamente tipada**, o que significa que você não precisa declarar o tipo de uma variável. Vamos ver alguns exemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dicionários\n",
    "Python fornece uma estrutura de dados muito útil para cientistas de dados (e programadores no geral) chamada de Dicionário. Você pode enchergar ela como um `Hash Map`, onde temos pares *chave-valor*. O grande diferencial é que esses pares podem ser de tipos diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condicionais, loops e escopo\n",
    "Códigos em Python não utilizam chaves para identificação de escopo. a identificação é feita a partir da **identação** do código. Para ilustrar, vamos fazer um código que detecta valores pares ou ímpares em uma lista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções e compreensão de lista\n",
    "\n",
    "Python trabalha com funções da mesma forma que outras linguagens. Você declara suas funções e as utiliza onde for necessário, passando os parâmetros corretos. \n",
    "Python ainda fornece uma facilidade para a manipulação de listas, chamada **compreensão de listas**. Nisso, você pode converter um `for` explícito em apenas uma linha de código. Vamos ver um exemplo de compreensão de listas com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisando e transformando dados com Spark\n",
    "\n",
    "Iremos usar a biblioteca de Ciência de Dados Distribuída chamada PySpark (versão Python do Spark). O Spark possui diversas funcionalidades implementadas para manipulação de dados e execução de *pipelines* de Aprendizado de Máquina em **Sistemas Distribuídos**. Um Sistema Distribuído é um que engloba vários computadores interconectados operando como uma única unidade. Sistemas assim são necessários para projetos de Big Data, pois o volume de dados excede a capacidade de uma única máquina processar. \n",
    "\n",
    "Computação distribuída foge do escopo deste Notebook, portanto iremos abstrair o conceito. A razão de trabalharmos com Spark é que esta é uma biblioteca extremamente simples e didática para ensinar as principais operações de ciência de dados. Spark é construído em Scala, porém fornece aplicações em diversas linguagens. Uma delas é Python, chamada PySpark. Spark fornece uma série de módulos:\n",
    "\n",
    "![Pilha de APIs, disponível em: https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/](https://izhangzhihao.github.io/assets/images/spark-05.png)\n",
    "\n",
    "Na aula de hoje veremos um pouco sobre `Dataframes` e `SparkSQL`, pois operam \"em conjunto\".\n",
    "\n",
    "**Dataframes** são estruturas tabulares, assim como os DataFrames de Pandas. A diferença, aqui, é que Dataframes em Spark são _distribuídos_ e construídos em cima de RDDs (a unidade básica e de mais baixo nível onde as demais APIs de Spark são construídas). \n",
    "\n",
    "**SparkSQL** é um conjunto de funcionalidades que são operadas em Dataframes. Veremos que podemos manipular Dataframes tanto programaticamente quanto por linguagem SQL. Além disso, SparkSQL oferece uma série de outras ferramentas para a realização de operações tabulares distribuídas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando e importando PySpark\n",
    "\n",
    "Para instalá-la, basta fazer o download  e seguir os passos em: http://spark.apache.org/downloads.html, ou instalar usando Pypi:\n",
    "\n",
    "\tpip install pyspark\n",
    "\n",
    "Para utilizá-la em conjunto com um Jupyter Notebook, você precisa baixar a biblioteca Jupyter.\n",
    "\n",
    "\tpip install jupyter\n",
    "    \n",
    "Depois, baixar também a biblioteca findspark\n",
    "\n",
    "\tpip install findspark\n",
    "\n",
    "E pronto! Agora, no começo de cada notebook você deverá importar tanto `findspark` quanto `pyspark`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O objeto SparkSession\n",
    "\n",
    "Para manipularmos Dataframes e utilizar as funções do SparkSQL, porém, precisamos criar um objeto `SparkSession`. O `SparkSession` é um objeto que serve como ponto de entrada para as APIs estruturadas do Spark. \n",
    "\n",
    "![SparkSession, disponível em: https://www.dcc.fc.up.pt/~edrdo/aulas/bdcc/classes/spark_arch/images/python_and_spark.png](https://www.dcc.fc.up.pt/~edrdo/aulas/bdcc/classes/spark_arch/images/python_and_spark.png)\n",
    "\n",
    "Para inicializar um SparkSession, importamos sua definição de `pyspark.sql` e criamos um objeto com `SparkSession.builder.getOrCreate()`. Esse método checa se já existe um `SparkSession` ativo, e se não existir, cria um novo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando Dataframes\n",
    "\n",
    "Dataframes são objetos tabulares distribuídos. Você pode criar um dataframe lendo arquivos csv, JSON, [ORC](https://orc.apache.org/) e [Parquet](https://github.com/apache/parquet-format). Vamos começar fazendo um exemplo de leitura simplificada de um arquivo csv. Vamos ler um conjunto de dados sobre vôos realizados nos Estados Unidos no ano de 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas\n",
    "\n",
    "O Spark pode fazer múltiplas leituras de um arquivo CSV para automaticamente tentar **inferir** qual é o _Schema_ do Dataframe, ou seja, qual é a tipagem de cada coluna e se ela pode ter ou não valores _null_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detectando valores errados\n",
    "\n",
    "Parece que o Spark não inferiu corretamente o tipo de algumas colunas! Isso pode acontecer se tivermos múltiplos tipos de dados numa única coluna. Quando isso acontece, ele atribui por padrão o tipo `string`. Vamos tentar detectar quais valores estão dando problema.\n",
    "\n",
    "Para isso, iremos utilizar duas funções de Dataframes do Spark: `select()` e `filter()`. Vamos ver um pouco de cada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select\n",
    "\n",
    "O comando `select()` opera em nível de **coluna**, filtrando quais colunas você quer exibir. Ele pode ainda criar novas colunas, com métodos que analisam valores de uma ou mais colunas linha a linha. É o equivalente ao comando `SELECT` de uma consulta SQL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter\n",
    "\n",
    "A função `filter()` opera em nível de **linhas**, e utiliza operações lógicas que retornam resultados _binários_. É similar ao comando `WHERE` de uma consulta SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando funções customizadas em `filter()`\n",
    "\n",
    "Para podermos entender o que está errado com os valores que deveriam ser `Integers` em algumas das colunas do nosso `Dataframe`, nós vamos primeiro ter que criar uma função que verifique se os valores de uma coluna são números ou não. O Spark fornece um mecanismo chamado `UDF` (_User-Defined Functions_) para que seus usuários criem funções customizadas. Para tal, precisamos importar o pacote `pyspark.sql.functions`, definir nossas funções e encapsulá-las em um `UDF` usando o seguinte formato:\n",
    "\n",
    "`F.udf(funcão, tipo_de_retorno)`\n",
    "\n",
    "Vamos criar uma função que verifica se um valor é inteiro ou não, e aplicar esta função em um filtro para exibir dados que **não** são inteiros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removendo valores errôneos\n",
    "\n",
    "Detectamos que em nosso `Dataframe`, valores nulos (`NaN`s) estão representados pelo String **NA**. Vamos tentar remover todas as linhas que tiverem valores **NA**.\n",
    "\n",
    "Para facilitar nossa vida, Spark fornece opções para a definição de valores nulos durante a leitura de arquivos. Vamos reler nosso _flights.csv_ definindo como `null` células da nossa tabela que tiverem o String **NA**.\n",
    "\n",
    "Uma outra opção seria filtrar o Dataframe coluna a coluna para valores `!= NA`, mas isso é um trabalho repetitivo e entediante que pode ser resolvido de uma maneira mais simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntando tabelas diferentes\n",
    "\n",
    "A função `join()` une dois Dataframes a partir de uma coluna em comum. Há vários tipos diferentes de joins no Spark:\n",
    "\n",
    "![Tipos de join. Disponível em: https://medium.com/bild-journal/pyspark-joins-explained-9c4fba124839](https://miro.medium.com/max/622/1*6d7MzkjxS0eBWjOJN5TaAQ.jpeg)\n",
    "\n",
    "Todos os tipos de join mostrados acima podem ser usados junto com o método. Vamos ver dois exemplos: `'left_outer'` e `'inner'`. Os demais tipos estão listados na [documentação](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.join.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removendo colunas desnecessárias e linhas duplicadas\n",
    "\n",
    "Algumas colunas de nosso Dataframe podem, na verdade, influenciar negativamente no treinamento de um modelo preditivo. Algumas informações podem de fato ser removidas. Para tal, iremos utilizar o método `drop()`.\n",
    "\n",
    "Só para nos precavermos, vamos remover também linhas duplicadas utilizando o método `distinct()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando e transformando colunas\n",
    "\n",
    "Vamos fazer mais algumas modificações que irão auxiliar futuramente nosso modelo a prever se um vôo irá atrasar ou não. Algoritmos de aprendizado de máquina não funcionam bem quando dados estão em dimensões diferentes entre colunas. É importante que façamos modificações para que as colunas estejam na mesma dimensão e assim uma não acabe introduzindo viés sobre a outra. \n",
    "\n",
    "Iremos trabalhar com normalização dos dados em algumas colunas aqui. Outras, que representam dados **categóricos** e não **contínuos**, serão manipulados na próxima aula. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização em _z-score_\n",
    "\n",
    "Esse tipo de normalização transforma os dados em valores que representam sua variação em relação à média. \n",
    "\n",
    "$$ z = \\frac{x_i - \\mu}{\\delta} $$\n",
    "\n",
    "![Normalização em z-score, disponível em: https://miro.medium.com/max/692/1*er9yh82tMZ85RWOSkKb-xA.png](https://miro.medium.com/max/692/1*er9yh82tMZ85RWOSkKb-xA.png)\n",
    "\n",
    "Para transformar nossos dados, vamos precisar definir uma `UDF` que realize esse cálculo. Vamos precisar também definir a média e o desvio padrão de nossas colunas. Podemos utilizar o comando `agg()` pra isso. Finalmente, precisamos criar colunas com esses valores transformados utilizando o método `withColumn()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizando Dados graficamente\n",
    "\n",
    "Infelizmente, Spark não suporta visualização de dados nativamente. Para visualizá-los, podemos:\n",
    "- Utilizar bibliotecas de terceiros, como a `pyspark_dist_explore` (não recomendado, última versão em 2019), ou\n",
    "- Retirar uma amostra dos dados e transformá-la em `Pandas Dataframes`, e aproveitar as bibliotecas `Matplotlib` ou `Seaborn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb071969cc081d156db4658a52cc12ff3302089485c0c8cb524fcf02c6362775"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
